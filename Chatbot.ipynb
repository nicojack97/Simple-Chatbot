{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGaxmagfhs9gkXCFTvrA0v"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oje-KwCqGSS9"
      },
      "source": [
        "\r\n",
        "# The following application is a simple chatbot, called Friday, which is able to respond to a series of simple\r\n",
        "# questions, using as training set a json file containing sample sentences about 19 topics.\r\n",
        "# The pre-processing of the sentences ha been done using the nltk module for natural language manipulation:\r\n",
        "# the sentences has been first tokenized and then stemmed with the Lancaster stemmer\r\n",
        "# The model used is a 2 layer, fully connected dense neural network first the model is trained on the json file data\r\n",
        "# and then it is used to classify the sentence coming from the input of the user, and respond accordingly.\r\n",
        "## GIACOMO PERONI\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnlAtuSH3DAa"
      },
      "source": [
        "\r\n",
        "!pip install tflearn\r\n",
        "import nltk\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "stemmer = LancasterStemmer()\r\n",
        "nltk.download('punkt')\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import tflearn\r\n",
        "import random\r\n",
        "import json\r\n",
        "\r\n",
        "with open('intents.json') as file:\r\n",
        "  data = json.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXDCkdonc2ew",
        "outputId": "fe713a11-a87c-4539-f393-2307b4b727b7"
      },
      "source": [
        "  ## DATA PREPROCESSING:\r\n",
        "  \r\n",
        "  words = []\r\n",
        "  labels = []\r\n",
        "  docs_x = []\r\n",
        "  docs_y =[]\r\n",
        "\r\n",
        "  for intent in data['intents']:\r\n",
        "    for pattern in intent['patterns']:\r\n",
        "      ## the tokenizing process breaks the sentences into single words\r\n",
        "      wrds = nltk.word_tokenize(pattern)\r\n",
        "      words.extend(wrds)\r\n",
        "      docs_x.append(wrds)\r\n",
        "      docs_y.append(intent['tag'])\r\n",
        "\r\n",
        "    if intent['tag'] not in labels:\r\n",
        "      labels.append(intent['tag'])\r\n",
        "\r\n",
        "  ## the stemming process extracts the root/base form of the worlds e.g goes, gone, go --> go\r\n",
        "  words = [stemmer.stem(w.lower()) for w in words if w not in '?']\r\n",
        "  words = sorted(list(set(words)))\r\n",
        "  ##print(words)\r\n",
        "\r\n",
        "  labels=sorted(labels)\r\n",
        "\r\n",
        "  ## preparing the sets needed for the model\r\n",
        "  training = []\r\n",
        "  output = []\r\n",
        "  out_empty = [0 for _ in range(len(labels))]\r\n",
        "\r\n",
        "  ## one hot encoding: translate every sentence into an array as long as the number of\r\n",
        "  ## words and having a 1 if the word at the i's index is contained in the sentence and 0 elsewhere \r\n",
        "  for x,doc in enumerate(docs_x):\r\n",
        "      bag= []\r\n",
        "    \r\n",
        "      wrds = [stemmer.stem(w) for w in doc]\r\n",
        "      for w in words:\r\n",
        "          if w in wrds:\r\n",
        "            bag.append(1)\r\n",
        "          else:\r\n",
        "            bag.append(0)\r\n",
        "\r\n",
        "##the output simply tells us for every sentence, what category (greetings, farewells, shop ecc...) it belongs to\r\n",
        "      output_row=out_empty[:]\r\n",
        "      output_row[labels.index(docs_y[x])] =1\r\n",
        "      training.append(bag)\r\n",
        "      output.append(output_row)\r\n",
        "\r\n",
        "  training = np.array(training)\r\n",
        "  output = np.array(output)\r\n",
        "\r\n",
        "## MODEL DEFINITION:  \r\n",
        "\r\n",
        "\r\n",
        "  ## this defines the model expectation about the shape and size of the input\r\n",
        "  net = tflearn.input_data(shape=[None,len(training[0])])\r\n",
        "## here we define the layers of the neural network\r\n",
        "  net = tflearn.fully_connected(net,8)\r\n",
        "  net = tflearn.fully_connected(net,8)\r\n",
        "  net = tflearn.fully_connected(net,len(output[0]), activation='softmax')\r\n",
        "  net = tflearn.regression(net)\r\n",
        "\r\n",
        "  model = tflearn.DNN(net)\r\n",
        "\r\n",
        "model.fit(training,output, n_epoch=300, batch_size=8 ,show_metric=True)\r\n",
        "\r\n",
        "\r\n",
        "## one hot encoding\r\n",
        "def bag_of_words(s,words):\r\n",
        "    bag = [0 for _ in range(len(words))]\r\n",
        "    s_words = nltk.word_tokenize(s)\r\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\r\n",
        "\r\n",
        "    for se in s_words:\r\n",
        "      for i,w in enumerate(words):\r\n",
        "        if w == se:\r\n",
        "          bag[i]=1\r\n",
        "    return np.array(bag)\r\n",
        "\r\n",
        "## CHATBOT FUNCTION:\r\n",
        "def chat():\r\n",
        "    print('Start talking with the bot (type quit to stop)')\r\n",
        "    while True:\r\n",
        "      inp = input('You:  ')\r\n",
        "      if inp.lower() == 'quit':\r\n",
        "        break \r\n",
        "      ## PREDICTING THE LABEL OF THE USER'S SENTENCE\r\n",
        "      ## remember the square brackets for the bag otherwyse we have dimensional problems\r\n",
        "      result = model.predict([bag_of_words(inp,words)])\r\n",
        "      result_index = np.argmax(result)\r\n",
        "      tag = labels[result_index]\r\n",
        "\r\n",
        "      ## defining a threshold for considering the answer \"valid\"\r\n",
        "      if result[:,result_index] > 0.6:\r\n",
        "        \r\n",
        "          #formulate an answer\r\n",
        "          for t in data['intents']:\r\n",
        "            if t['tag'] == tag:\r\n",
        "              answers = t['responses']\r\n",
        "          print(random.choice(answers))\r\n",
        "      else:\r\n",
        "          print(\"sorry, I didn't understand, can you repeat?\")\r\n",
        "\r\n",
        "chat()\r\n",
        "\r\n",
        "\r\n",
        "## The model has some issues like the fact that the model needs to be trained everytime,\r\n",
        "## this could be fixed saving the model in the repository and loading it every time, other problems\r\n",
        "## regard the limited amount of data disposable and the generality of the answers, aspects\r\n",
        "## that can be improved with some solutions that are beyond the scope of this application\r\n",
        "      \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Step: 2999  | total loss: \u001b[1m\u001b[32m0.29318\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 300 | loss: 0.29318 - acc: 0.9735 -- iter: 72/75\n",
            "Training Step: 3000  | total loss: \u001b[1m\u001b[32m0.28420\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 300 | loss: 0.28420 - acc: 0.9762 -- iter: 75/75\n",
            "--\n",
            "Start talking with the bot (type quit to stop)\n",
            "You:  quit\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}